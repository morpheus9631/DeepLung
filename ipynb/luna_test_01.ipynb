{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections.abc\n",
    "import errno\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import scipy\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from importlib import import_module\n",
    "from os import listdir\n",
    "from os.path import dirname, exists, isdir, join\n",
    "from pynvml import *\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.backends import cudnn\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "RootPath = r''\n",
    "\n",
    "if RootPath not in sys.path: sys.path.insert(0, RootPath)\n",
    "from configs.config03 import get_cfg_defaults\n",
    "\n",
    "CodePath = join(RootPath, 'code')\n",
    "if CodePath not in sys.path: sys.path.insert(1, CodePath)\n",
    "from models import res18_se\n",
    "\n",
    "display(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(**kwargs):\n",
    "    parser = argparse.ArgumentParser(description='Luna nodule detection by pyTorch')\n",
    "    parser.add_argument(\"--cfg\", type=str, default=\"configs\\\\config03_win.yaml\", \n",
    "                        help=\"Configuration\")\n",
    "    return parser.parse_args(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeNotExistDir(dirPath):\n",
    "    if not exists(dirPath):\n",
    "        pathlib.Path(dirPath).mkdir(parents=True, exist_ok=True) \n",
    "    return exists(dirPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFreeId():\n",
    "    \n",
    "    nvmlInit()\n",
    "    def getFreeRatio(id):\n",
    "        handle = nvmlDeviceGetHandleByIndex(id)\n",
    "        use = nvmlDeviceGetUtilizationRates(handle)\n",
    "        ratio = 0.5*(float(use.gpu+float(use.memory)))\n",
    "        return ratio\n",
    "\n",
    "    deviceCount = nvmlDeviceGetCount()\n",
    "    available = []\n",
    "    for i in range(deviceCount):\n",
    "        if getFreeRatio(i)<70:\n",
    "            available.append(i)\n",
    "    gpus = ''\n",
    "    for g in available:\n",
    "        gpus = gpus+str(g)+','\n",
    "    gpus = gpus[:-1]\n",
    "    return gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setgpu(gpuinput):\n",
    "    freeids = getFreeId()\n",
    "    if gpuinput=='all':\n",
    "        gpus = freeids\n",
    "    else:\n",
    "        gpus = gpuinput\n",
    "        if any([g not in freeids for g in gpus.split(',')]):\n",
    "            raise ValueError('gpu'+g+'is being used')\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "    return gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitComb():\n",
    "    def __init__(self, side_len, max_stride, stride, margin, pad_value):\n",
    "        self.side_len = side_len\n",
    "        self.max_stride = max_stride\n",
    "        self.stride = stride\n",
    "        self.margin = margin\n",
    "        self.pad_value = pad_value\n",
    "        \n",
    "    def split(self, data, side_len = None, max_stride = None, margin = None):\n",
    "        if side_len==None:\n",
    "            side_len = self.side_len\n",
    "        if max_stride == None:\n",
    "            max_stride = self.max_stride\n",
    "        if margin == None:\n",
    "            margin = self.margin\n",
    "        \n",
    "        assert(side_len > margin)\n",
    "        assert(side_len % max_stride == 0)\n",
    "        assert(margin % max_stride == 0)\n",
    "\n",
    "        splits = []\n",
    "        _, z, h, w = data.shape\n",
    "\n",
    "        nz = int(np.ceil(float(z) / side_len))\n",
    "        nh = int(np.ceil(float(h) / side_len))\n",
    "        nw = int(np.ceil(float(w) / side_len))\n",
    "        \n",
    "        nzhw = [nz,nh,nw]\n",
    "        self.nzhw = nzhw\n",
    "        \n",
    "        pad = [ [0, 0],\n",
    "                [margin, nz * side_len - z + margin],\n",
    "                [margin, nh * side_len - h + margin],\n",
    "                [margin, nw * side_len - w + margin]]\n",
    "        data = np.pad(data, pad, 'edge')\n",
    "\n",
    "        for iz in range(nz):\n",
    "            for ih in range(nh):\n",
    "                for iw in range(nw):\n",
    "                    sz = iz * side_len\n",
    "                    ez = (iz + 1) * side_len + 2 * margin\n",
    "                    sh = ih * side_len\n",
    "                    eh = (ih + 1) * side_len + 2 * margin\n",
    "                    sw = iw * side_len\n",
    "                    ew = (iw + 1) * side_len + 2 * margin\n",
    "\n",
    "                    split = data[np.newaxis, :, sz:ez, sh:eh, sw:ew]\n",
    "                    splits.append(split)\n",
    "\n",
    "        splits = np.concatenate(splits, 0)\n",
    "        return splits,nzhw\n",
    "\n",
    "    def combine(self, output, nzhw = None, side_len=None, stride=None, margin=None):\n",
    "        \n",
    "        if side_len==None:\n",
    "            side_len = self.side_len\n",
    "        if stride == None:\n",
    "            stride = self.stride\n",
    "        if margin == None:\n",
    "            margin = self.margin\n",
    "        if nzhw is None:\n",
    "            nz = self.nz\n",
    "            nh = self.nh\n",
    "            nw = self.nw\n",
    "        else:\n",
    "            nz,nh,nw = nzhw\n",
    "        assert(side_len % stride == 0)\n",
    "        assert(margin % stride == 0)\n",
    "        side_len = int(side_len/stride)\n",
    "        margin = int(margin/stride)\n",
    "\n",
    "        splits = []\n",
    "        for i in range(len(output)):\n",
    "            splits.append(output[i])\n",
    "\n",
    "        output = -1000000 * np.ones((\n",
    "            nz * side_len,\n",
    "            nh * side_len,\n",
    "            nw * side_len,\n",
    "            splits[0].shape[3],\n",
    "            splits[0].shape[4]), np.float32)\n",
    "\n",
    "        idx = 0\n",
    "        for iz in range(nz):\n",
    "            for ih in range(nh):\n",
    "                for iw in range(nw):\n",
    "                    sz = iz * side_len\n",
    "                    ez = (iz + 1) * side_len\n",
    "                    sh = ih * side_len\n",
    "                    eh = (ih + 1) * side_len\n",
    "                    sw = iw * side_len\n",
    "                    ew = (iw + 1) * side_len\n",
    "\n",
    "                    split = splits[idx][margin:margin + side_len, margin:margin + side_len, margin:margin + side_len]\n",
    "                    output[sz:ez, sh:eh, sw:ew] = split\n",
    "                    idx += 1\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LungNodule3Ddetector(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir, split_path, config, phase = 'train',split_comber=None):\n",
    "        assert(phase == 'train' or phase == 'val' or phase == 'test')\n",
    "        self.phase = phase\n",
    "        \n",
    "        self.max_stride = config['max_stride']\n",
    "        self.stride = config['stride']       \n",
    "        \n",
    "        sizelim  = config['sizelim'] /config['reso']\n",
    "        sizelim2 = config['sizelim2']/config['reso']\n",
    "        sizelim3 = config['sizelim3']/config['reso']\n",
    "        \n",
    "        self.blacklist = config['blacklist']\n",
    "        self.isScale = config['aug_scale']\n",
    "        self.r_rand  = config['r_rand_crop']\n",
    "        self.augtype = config['augtype']\n",
    "        self.pad_value = config['pad_value']\n",
    "\n",
    "        self.split_comber = split_comber\n",
    "        #idcs = np.load(split_path)\n",
    "        idcs = split_path\n",
    "\n",
    "        if phase != 'test':\n",
    "            idcs = [f for f in idcs if (f not in self.blacklist)]\n",
    "\n",
    "        self.filenames = [os.path.join(data_dir, '%s_clean.npy' % idx) for idx in idcs]\n",
    "        # display(self.filenames)\n",
    "        \n",
    "        labels = []\n",
    "        for idx in idcs:\n",
    "            l = np.load(os.path.join(data_dir, '%s_label.npy' %idx))\n",
    "            if np.all(l==0):\n",
    "                l=np.array([])\n",
    "            labels.append(l)\n",
    "        # display(labels)\n",
    "            \n",
    "        self.sample_bboxes = labels\n",
    "        if self.phase != 'test':\n",
    "            self.bboxes = []\n",
    "\n",
    "            for i, l in enumerate(labels):\n",
    "                if len(l) > 0 :\n",
    "                    for t in l:\n",
    "                        if t[3]>sizelim:\n",
    "                            self.bboxes+=[[np.concatenate([[i],t])]]\n",
    "                        if t[3]>sizelim2:\n",
    "                            self.bboxes+=[[np.concatenate([[i],t])]]*2\n",
    "                        if t[3]>sizelim3:\n",
    "                            self.bboxes+=[[np.concatenate([[i],t])]]*4\n",
    "            self.bboxes = np.concatenate(self.bboxes,axis = 0)\n",
    "\n",
    "        self.crop = Crop(config)\n",
    "        self.label_mapping = LabelMapping(config, self.phase)\n",
    "\n",
    "    def __getitem__(self, idx,split=None):\n",
    "        t = time.time()\n",
    "        np.random.seed(int(str(t%1)[2:7]))#seed according to time\n",
    "\n",
    "        isRandomImg  = False\n",
    "        if self.phase !='test':\n",
    "            if idx>=len(self.bboxes):\n",
    "                isRandom = True\n",
    "                idx = idx%len(self.bboxes)\n",
    "                isRandomImg = np.random.randint(2)\n",
    "            else:\n",
    "                isRandom = False\n",
    "        else:\n",
    "            isRandom = False\n",
    "        \n",
    "        if self.phase != 'test':\n",
    "            if not isRandomImg:\n",
    "                bbox = self.bboxes[idx]\n",
    "                filename = self.filenames[int(bbox[0])]\n",
    "                imgs = np.load(filename)\n",
    "                bboxes = self.sample_bboxes[int(bbox[0])]\n",
    "                isScale = self.augtype['scale'] and (self.phase=='train')\n",
    "                sample, target, bboxes, coord = self.crop(imgs, bbox[1:], bboxes,isScale,isRandom)\n",
    "                if self.phase=='train' and not isRandom:\n",
    "                     sample, target, bboxes, coord = augment(sample, target, bboxes, coord,\n",
    "                        ifflip = self.augtype['flip'], ifrotate=self.augtype['rotate'], ifswap = self.augtype['swap'])\n",
    "            else:\n",
    "                randimid = np.random.randint(len(self.filenames))\n",
    "                filename = self.filenames[randimid]\n",
    "                imgs = np.load(filename)\n",
    "                bboxes = self.sample_bboxes[randimid]\n",
    "                isScale = self.augtype['scale'] and (self.phase=='train')\n",
    "                sample, target, bboxes, coord = self.crop(imgs, [], bboxes, isScale=False, isRand=True)\n",
    "            label = self.label_mapping(sample.shape[1:], target, bboxes)\n",
    "            sample = (sample.astype(np.float32)-128)/128\n",
    "            return torch.from_numpy(sample), torch.from_numpy(label), coord\n",
    "        else:\n",
    "            imgs = np.load(self.filenames[idx])\n",
    "            bboxes = self.sample_bboxes[idx]\n",
    "            nz, nh, nw = imgs.shape[1:]\n",
    "            pz = int(np.ceil(float(nz) / self.stride)) * self.stride\n",
    "            ph = int(np.ceil(float(nh) / self.stride)) * self.stride\n",
    "            pw = int(np.ceil(float(nw) / self.stride)) * self.stride\n",
    "            imgs = np.pad(\n",
    "                imgs, \n",
    "                [[0,0], [0, pz-nz], [0, ph-nh], [0, pw-nw]], \n",
    "                'constant',\n",
    "                constant_values = self.pad_value\n",
    "            )\n",
    "            \n",
    "            xx, yy, zz = np.meshgrid(\n",
    "                np.linspace(-0.5, 0.5, int(imgs.shape[1]/self.stride)),\n",
    "                np.linspace(-0.5, 0.5, int(imgs.shape[2]/self.stride)),\n",
    "                np.linspace(-0.5, 0.5, int(imgs.shape[3]/self.stride)),\n",
    "                indexing ='ij'\n",
    "            )\n",
    "            coord = np.concatenate([xx[np.newaxis,...], yy[np.newaxis,...],zz[np.newaxis,:]],0).astype('float32')\n",
    "            \n",
    "            imgs, nzhw = self.split_comber.split(imgs)\n",
    "            \n",
    "            coord2, nzhw2 = self.split_comber.split(\n",
    "                coord,\n",
    "                side_len = int(self.split_comber.side_len/self.stride),\n",
    "                max_stride = int(self.split_comber.max_stride/self.stride),\n",
    "                margin = int(self.split_comber.margin/self.stride)\n",
    "            )\n",
    "            \n",
    "            assert np.all(nzhw==nzhw2)\n",
    "            imgs = (imgs.astype(np.float32)-128)/128\n",
    "            return torch.from_numpy(imgs), bboxes, torch.from_numpy(coord2), np.array(nzhw)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.phase == 'train':\n",
    "            return int(len(self.bboxes)/(1-self.r_rand))\n",
    "        elif self.phase =='val':\n",
    "            return len(self.bboxes)\n",
    "        else:\n",
    "            return len(self.sample_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(sample, target, bboxes, coord, ifflip=True, ifrotate=True, ifswap=True):\n",
    "    #                     angle1 = np.random.rand()*180\n",
    "    if ifrotate:\n",
    "        validrot = False\n",
    "        counter = 0\n",
    "        while not validrot:\n",
    "            newtarget = np.copy(target)\n",
    "            angle1 = (np.random.rand() - 0.5) * 20\n",
    "            size = np.array(sample.shape[2:4]).astype('float')\n",
    "            rotmat = np.array([[np.cos(angle1 / 180 * np.pi), -np.sin(angle1 / 180 * np.pi)],\n",
    "                               [np.sin(angle1 / 180 * np.pi), np.cos(angle1 / 180 * np.pi)]])\n",
    "            newtarget[1:3] = np.dot(rotmat, target[1:3] - size / 2) + size / 2\n",
    "            if np.all(newtarget[:3] > target[3]) and np.all(newtarget[:3] < np.array(sample.shape[1:4]) - newtarget[3]):\n",
    "                validrot = True\n",
    "                target = newtarget\n",
    "                sample = rotate(sample, angle1, axes=(2, 3), reshape=False)\n",
    "                coord = rotate(coord, angle1, axes=(2, 3), reshape=False)\n",
    "                for box in bboxes:\n",
    "                    box[1:3] = np.dot(rotmat, box[1:3] - size / 2) + size / 2\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter == 3:\n",
    "                    break\n",
    "    if ifswap:\n",
    "        if sample.shape[1] == sample.shape[2] and sample.shape[1] == sample.shape[3]:\n",
    "            axisorder = np.random.permutation(3)\n",
    "            sample = np.transpose(sample, np.concatenate([[0], axisorder + 1]))\n",
    "            coord = np.transpose(coord, np.concatenate([[0], axisorder + 1]))\n",
    "            target[:3] = target[:3][axisorder]\n",
    "            bboxes[:, :3] = bboxes[:, :3][:, axisorder]\n",
    "\n",
    "    if ifflip:\n",
    "        #         flipid = np.array([np.random.randint(2),np.random.randint(2),np.random.randint(2)])*2-1\n",
    "        flipid = np.array([1, np.random.randint(2), np.random.randint(2)]) * 2 - 1\n",
    "        sample = np.ascontiguousarray(sample[:, ::flipid[0], ::flipid[1], ::flipid[2]])\n",
    "        coord = np.ascontiguousarray(coord[:, ::flipid[0], ::flipid[1], ::flipid[2]])\n",
    "        for ax in range(3):\n",
    "            if flipid[ax] == -1:\n",
    "                target[ax] = np.array(sample.shape[ax + 1]) - target[ax]\n",
    "                bboxes[:, ax] = np.array(sample.shape[ax + 1]) - bboxes[:, ax]\n",
    "    return sample, target, bboxes, coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crop(object):\n",
    "    def __init__(self, config):\n",
    "        self.crop_size = config['crop_size']\n",
    "        self.bound_size = config['bound_size']\n",
    "        self.stride = config['stride']\n",
    "        self.pad_value = config['pad_value']\n",
    "\n",
    "    def __call__(self, imgs, target, bboxes, isScale=False, isRand=False):\n",
    "        if isScale:\n",
    "            radiusLim = [8., 100.]\n",
    "            scaleLim = [0.75, 1.25]\n",
    "            scaleRange = [np.min([np.max([(radiusLim[0] / target[3]), scaleLim[0]]), 1])\n",
    "                , np.max([np.min([(radiusLim[1] / target[3]), scaleLim[1]]), 1])]\n",
    "            scale = np.random.rand() * (scaleRange[1] - scaleRange[0]) + scaleRange[0]\n",
    "            crop_size = (np.array(self.crop_size).astype('float') / scale).astype('int')\n",
    "        else:\n",
    "            crop_size = self.crop_size\n",
    "        bound_size = self.bound_size\n",
    "        target = np.copy(target)\n",
    "        bboxes = np.copy(bboxes)\n",
    "\n",
    "        start = []\n",
    "        for i in range(3):\n",
    "            if not isRand:\n",
    "                r = target[3] / 2\n",
    "                s = np.floor(target[i] - r) + 1 - bound_size\n",
    "                e = np.ceil(target[i] + r) + 1 + bound_size - crop_size[i]\n",
    "            else:\n",
    "                s = np.max([imgs.shape[i + 1] - crop_size[i] / 2, imgs.shape[i + 1] / 2 + bound_size])\n",
    "                e = np.min([crop_size[i] / 2, imgs.shape[i + 1] / 2 - bound_size])\n",
    "                target = np.array([np.nan, np.nan, np.nan, np.nan])\n",
    "            if s > e:\n",
    "                start.append(int(np.random.randint(e, s)))  # !\n",
    "            else:\n",
    "                start.append(int(target[i] - crop_size[i] / 2 + np.random.randint(-bound_size / 2, bound_size / 2)))\n",
    "\n",
    "        normstart = np.array(start).astype('float32') / np.array(imgs.shape[1:]) - 0.5\n",
    "        normsize = np.array(crop_size).astype('float32') / np.array(imgs.shape[1:])\n",
    "        xx, yy, zz = np.meshgrid(\n",
    "            np.linspace(normstart[0], normstart[0] + normsize[0], int(self.crop_size[0]/self.stride)),\n",
    "            np.linspace(normstart[1], normstart[1] + normsize[1], int(self.crop_size[1]/self.stride)),\n",
    "            np.linspace(normstart[2], normstart[2] + normsize[2], int(self.crop_size[2]/self.stride)),\n",
    "            indexing='ij')\n",
    "        \n",
    "        coord = np.concatenate([xx[np.newaxis, ...], yy[np.newaxis, ...], zz[np.newaxis, :]], 0).astype('float32')\n",
    "\n",
    "        pad = []\n",
    "        pad.append([0, 0])\n",
    "        for i in range(3):\n",
    "            leftpad = max(0, -start[i])\n",
    "            rightpad = max(0, start[i] + crop_size[i] - imgs.shape[i + 1])\n",
    "            pad.append([leftpad, rightpad])\n",
    "        crop = imgs[:,\n",
    "               max(start[0], 0):min(start[0] + crop_size[0], imgs.shape[1]),\n",
    "               max(start[1], 0):min(start[1] + crop_size[1], imgs.shape[2]),\n",
    "               max(start[2], 0):min(start[2] + crop_size[2], imgs.shape[3])]\n",
    "        crop = np.pad(crop, pad, 'constant', constant_values=self.pad_value)\n",
    "        for i in range(3):\n",
    "            target[i] = target[i] - start[i]\n",
    "        for i in range(len(bboxes)):\n",
    "            for j in range(3):\n",
    "                bboxes[i][j] = bboxes[i][j] - start[j]\n",
    "\n",
    "        if isScale:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                crop = zoom(crop, [1, scale, scale, scale], order=1)\n",
    "            newpad = self.crop_size[0] - crop.shape[1:][0]\n",
    "            if newpad < 0:\n",
    "                crop = crop[:, :-newpad, :-newpad, :-newpad]\n",
    "            elif newpad > 0:\n",
    "                pad2 = [[0, 0], [0, newpad], [0, newpad], [0, newpad]]\n",
    "                crop = np.pad(crop, pad2, 'constant', constant_values=self.pad_value)\n",
    "            for i in range(4):\n",
    "                target[i] = target[i] * scale\n",
    "            for i in range(len(bboxes)):\n",
    "                for j in range(4):\n",
    "                    bboxes[i][j] = bboxes[i][j] * scale\n",
    "        return crop, target, bboxes, coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelMapping(object):\n",
    "    def __init__(self, config, phase):\n",
    "        self.stride = np.array(config['stride'])\n",
    "        self.num_neg = int(config['num_neg'])\n",
    "        self.th_neg = config['th_neg']\n",
    "        self.anchors = np.asarray(config['anchors'])\n",
    "        self.phase = phase\n",
    "        \n",
    "        if phase == 'train':\n",
    "            self.th_pos = config['th_pos_train']\n",
    "        elif phase == 'val':\n",
    "            self.th_pos = config['th_pos_val']\n",
    "\n",
    "    def __call__(self, input_size, target, bboxes):\n",
    "        stride = self.stride\n",
    "        num_neg = self.num_neg\n",
    "        th_neg = self.th_neg\n",
    "        anchors = self.anchors\n",
    "        th_pos = self.th_pos\n",
    "        struct = generate_binary_structure(3, 1)\n",
    "\n",
    "        output_size = []\n",
    "        for i in range(3):\n",
    "            assert (input_size[i] % stride == 0)\n",
    "            output_size.append(int(input_size[i] / stride))\n",
    "\n",
    "        label = np.zeros(output_size + [len(anchors), 5], np.float32)\n",
    "        offset = ((stride.astype('float')) - 1) / 2\n",
    "        oz = np.arange(offset, offset + stride * (output_size[0] - 1) + 1, stride)\n",
    "        oh = np.arange(offset, offset + stride * (output_size[1] - 1) + 1, stride)\n",
    "        ow = np.arange(offset, offset + stride * (output_size[2] - 1) + 1, stride)\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            for i, anchor in enumerate(anchors):\n",
    "                iz, ih, iw = select_samples(bbox, anchor, th_neg, oz, oh, ow)\n",
    "                label[iz, ih, iw, i, 0] = 1\n",
    "                label[:, :, :, i, 0] = binary_dilation(label[:, :, :, i, 0].astype('bool'), structure=struct,\n",
    "                                                       iterations=1).astype('float32')\n",
    "\n",
    "        label = label - 1\n",
    "\n",
    "        if self.phase == 'train' and self.num_neg > 0:\n",
    "            neg_z, neg_h, neg_w, neg_a = np.where(label[:, :, :, :, 0] == -1)\n",
    "            neg_idcs = random.sample(range(len(neg_z)), min(num_neg, len(neg_z)))\n",
    "            neg_z, neg_h, neg_w, neg_a = neg_z[neg_idcs], neg_h[neg_idcs], neg_w[neg_idcs], neg_a[neg_idcs]\n",
    "            label[:, :, :, :, 0] = 0\n",
    "            label[neg_z, neg_h, neg_w, neg_a, 0] = -1\n",
    "\n",
    "        if np.isnan(target[0]):\n",
    "            return label\n",
    "        iz, ih, iw, ia = [], [], [], []\n",
    "        for i, anchor in enumerate(anchors):\n",
    "            iiz, iih, iiw = select_samples(target, anchor, th_pos, oz, oh, ow)\n",
    "            iz.append(iiz)\n",
    "            ih.append(iih)\n",
    "            iw.append(iiw)\n",
    "            ia.append(i * np.ones((len(iiz),), np.int64))\n",
    "        iz = np.concatenate(iz, 0)\n",
    "        ih = np.concatenate(ih, 0)\n",
    "        iw = np.concatenate(iw, 0)\n",
    "        ia = np.concatenate(ia, 0)\n",
    "        flag = True\n",
    "        if len(iz) == 0:\n",
    "            pos = []\n",
    "            for i in range(3):\n",
    "                pos.append(max(0, int(np.round((target[i] - offset) / stride))))\n",
    "            idx = np.argmin(np.abs(np.log(target[3] / anchors)))\n",
    "            pos.append(idx)\n",
    "            flag = False\n",
    "        else:\n",
    "            idx = random.sample(range(len(iz)), 1)[0]\n",
    "            pos = [iz[idx], ih[idx], iw[idx], ia[idx]]\n",
    "        dz = (target[0] - oz[pos[0]]) / anchors[pos[3]]\n",
    "        dh = (target[1] - oh[pos[1]]) / anchors[pos[3]]\n",
    "        dw = (target[2] - ow[pos[2]]) / anchors[pos[3]]\n",
    "        dd = np.log(target[3] / anchors[pos[3]])\n",
    "        label[pos[0], pos[1], pos[2], pos[3], :] = [1, dz, dh, dw, dd]\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    try:\n",
    "        from collections.abc import Iterable\n",
    "    except ModuleNotFoundError:\n",
    "        from collections import Iterable\n",
    "    \n",
    "    if torch.is_tensor(batch[0]):\n",
    "        return [b.unsqueeze(0) for b in batch]\n",
    "    elif isinstance(batch[0], np.ndarray):\n",
    "        return batch\n",
    "    elif isinstance(batch[0], int):\n",
    "        return torch.LongTensor(batch)\n",
    "    # elif isinstance(batch[0], collections.Iterable):\n",
    "    elif isinstance(batch[0], Iterable):\n",
    "        transposed = zip(*batch)\n",
    "        return [collate(samples) for samples in transposed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_loader, net, get_pbb, save_dir, config, numPerRun=1):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    save_dir = os.path.join(save_dir, 'bbox')\n",
    "    if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "    \n",
    "    net.eval()\n",
    "    namelist = []\n",
    "    split_comber = data_loader.dataset.split_comber\n",
    "    \n",
    "    for i_name, (data, target, coord, nzhw) in enumerate(data_loader):\n",
    "        target = [np.asarray(t, np.float32) for t in target]\n",
    "        lbb  = target[0]\n",
    "        nzhw = nzhw[0]\n",
    "        \n",
    "        # name = data_loader.dataset.filenames[i_name].split('-')[0].split('/')[-1].split('_clean')[0]\n",
    "        path, name = os.path.split(data_loader.dataset.filenames[i_name])\n",
    "        name = name.split('_clean')[0]\n",
    "        namelist.append(name)\n",
    "\n",
    "        data = data[0][0]\n",
    "        coord  = coord[0][0]\n",
    "        isfeat = False\n",
    "        \n",
    "        if 'output_feature' in config:\n",
    "            if config['output_feature']:\n",
    "                isfeat = True\n",
    "        print(); print(data.size())\n",
    "\n",
    "        n_per_run = numPerRun\n",
    "        splitlist = list(range(0,len(data)+1, n_per_run))\n",
    "        \n",
    "        if splitlist[-1] != len(data):\n",
    "            splitlist.append(len(data))\n",
    "\n",
    "        outputlist = []\n",
    "        featurelist = []\n",
    "\n",
    "        for i in range(len(splitlist)-1):\n",
    "            input = Variable(data[splitlist[i]:splitlist[i+1]].cuda())\n",
    "            inputcoord = Variable(coord[splitlist[i]:splitlist[i+1]].cuda())\n",
    "            if isfeat:\n",
    "                with torch.no_grad():\n",
    "                    output,feature = net(input,inputcoord)\n",
    "                featurelist.append(feature.data.cpu().numpy())\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    output = net(input, inputcoord)\n",
    "            outputlist.append(output.data.cpu().numpy())\n",
    "            del input, inputcoord, output\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        output = np.concatenate(outputlist,0)\n",
    "        output = split_comber.combine(output,nzhw=nzhw)\n",
    "        \n",
    "        if isfeat:\n",
    "            feature = np.concatenate(featurelist,0).transpose([0,2,3,4,1])[:,:,:,:,:,np.newaxis]\n",
    "            feature = split_comber.combine(feature,sidelen)[...,0]\n",
    "\n",
    "        thresh = -3\n",
    "        pbb,mask = get_pbb(output,thresh,ismask=True)\n",
    "\n",
    "        if isfeat:\n",
    "            feature_selected = feature[mask[0],mask[1],mask[2]]\n",
    "            np.save(os.path.join(save_dir, name+'_feature.npy'), feature_selected)\n",
    "            \n",
    "        #tp,fp,fn,_ = acc(pbb,lbb,0,0.1,0.1)\n",
    "        #print([len(tp),len(fp),len(fn)])\n",
    "        print([i_name, name, data_loader.dataset.filenames[i_name]])\n",
    "\n",
    "        np.save(os.path.join(save_dir, name+'_pbb.npy'), pbb)\n",
    "        np.save(os.path.join(save_dir, name+'_lbb.npy'), lbb)\n",
    "        \n",
    "    np.save(os.path.join(save_dir, 'namelist.npy'), namelist)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    print('elapsed time is %3.2f seconds' % (end_time - start_time))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args(args=[])\n",
    "print(args)\n",
    "    \n",
    "bestLoss = 1000\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.set_device(0)\n",
    "    \n",
    "path_root = RootPath\n",
    "path_conf = join(path_root, args.cfg)\n",
    "    \n",
    "CFG = get_cfg_defaults()\n",
    "CFG.merge_from_file(path_conf)\n",
    "CFG.freeze()\n",
    "print(); print(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = import_module(CFG.TEST.MODEL)\n",
    "config, net, loss, get_pbb = model.get_model()\n",
    "\n",
    "path_ckpt = CFG.TEST.DIR_CKPT\n",
    "file_ckpt = CFG.TEST.FILE_CKPT\n",
    "\n",
    "if file_ckpt:\n",
    "    abspath_ckpt = join(path_ckpt, file_ckpt)\n",
    "    if exists(abspath_ckpt):\n",
    "        print(\"\\nLoad: '{}'\".format(abspath_ckpt))\n",
    "        pretrained = torch.load(abspath_ckpt)\n",
    "        net.load_state_dict(pretrained['state_dict'])\n",
    "    else:\n",
    "        print(\"\\n'{}' not exist.\".format(abspath_ckpt))\n",
    "\n",
    "display(net.state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU \n",
    "gpus = setgpu(CFG.TEST.GPU)\n",
    "net  = net.cuda()\n",
    "loss = loss.cuda()\n",
    "cudnn.benchmark = True\n",
    "net = DataParallel(net)\n",
    "\n",
    "n_gpu = len(gpus.split(','))\n",
    "print('Num GPU:', n_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = join(path_root, CFG.TEST.DIR_SAVE)\n",
    "isExist = makeNotExistDir(path_save)\n",
    "print(\"Save dir: '{}' exist? {}\".format(path_save, (\"True\" if isExist else \"False\")))\n",
    "\n",
    "path_data = join(path_root, CFG.TEST.DIR_DATA)\n",
    "isExist = makeNotExistDir(path_data)\n",
    "print(\"Data dir: '{}' exist? {}\".format(path_data, (\"True\" if isExist else \"False\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNameList_Csv(csvPathFile, dataPath):\n",
    "    selected_names = pd.read_csv(csvPathFile).name.tolist()\n",
    "\n",
    "    name_list = []\n",
    "    for name in selected_names:\n",
    "        name_str = f'{name:03d}'\n",
    "        f_abspath = join(dataPath, name_str+'_label.npy')\n",
    "        #print(f_abspath)\n",
    "        if exists(f_abspath):\n",
    "            name_list.append(name_str)\n",
    "    return name_list\n",
    "\n",
    "def getNameList_Dir(dataPath):\n",
    "    series_name = pd.Series(glob(dataPath + \"\\\\*label.npy\")).apply(\n",
    "                lambda x: x.split(\"_label.npy\")[0].split(\"\\\\\")[-1])\n",
    "    return series_name.values.tolist()\n",
    "\n",
    "if CFG.TEST.NAMELIST:\n",
    "    abspath_namelist = join(CFG.TEST.DIR_CSV, CFG.TEST.NAMELIST)\n",
    "    isExist = exists(abspath_namelist)\n",
    "    print(\"load '{}' ? {}\".format(abspath_namelist, isExist))\n",
    "    if isExist:\n",
    "        luna_test = getNameList_Csv(abspath_namelist, path_data)\n",
    "    else:\n",
    "        luna_test = getNameList_Dir(path_data)\n",
    "else:\n",
    "    luna_test = getNameList_Dir(path_data)\n",
    "\n",
    "print('\\nLuna test:', luna_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Test\n",
    "margin = 32\n",
    "sidelen = 144\n",
    "\n",
    "stride = config['stride']\n",
    "max_stride = config['max_stride']\n",
    "pad_value = config['pad_value']\n",
    "\n",
    "split_comber = SplitComb(\n",
    "    sidelen,\n",
    "    max_stride,\n",
    "    stride,\n",
    "    margin,\n",
    "    pad_value\n",
    ")\n",
    "\n",
    "dataset = LungNodule3Ddetector(\n",
    "    path_data,\n",
    "    luna_test,\n",
    "    config,\n",
    "    phase='test',\n",
    "    split_comber=split_comber\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size = 1,\n",
    "    shuffle = False,\n",
    "    num_workers = 0,\n",
    "    collate_fn = collate,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "len_dset = dataset.__len__()\n",
    "print('Dataset len:', len_dset)\n",
    "\n",
    "imgs, bboxes, coord2, nzhw = dataset[0]\n",
    "\n",
    "print('\\nImage:',  imgs.shape)\n",
    "print('bboxes:', bboxes)\n",
    "print('coord2:', coord2.shape)\n",
    "print('nzhw:',   nzhw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_strt = datetime.now()\n",
    "print('Start: {}'.format(dt_strt))\n",
    "       \n",
    "test(test_loader, net, get_pbb, path_save, config)\n",
    "\n",
    "dt_stop = datetime.now()\n",
    "print('\\nStop : {}'.format(dt_stop))\n",
    "\n",
    "dt_diff = dt_stop - dt_strt\n",
    "print('Diff : {}'.format(dt_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
