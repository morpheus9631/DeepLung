{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections.abc\n",
    "import errno\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import random\n",
    "import scipy\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from importlib import import_module\n",
    "from os import listdir\n",
    "from os.path import abspath, dirname, exists, isdir, join\n",
    "from pynvml import *\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.ndimage.morphology import binary_dilation, generate_binary_structure\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.backends import cudnn\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "RootPath = abspath('..')    # For jupyter\n",
    "if RootPath not in sys.path: sys.path.insert(0, RootPath)\n",
    "display(sys.path)\n",
    "\n",
    "from models.res18_se import *\n",
    "from configs.config03 import get_cfg_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(**kwargs):\n",
    "    parser = argparse.ArgumentParser(description='Luna nodule detection by pyTorch')\n",
    "    parser.add_argument(\"--cfg\", type=str, default=\"configs\\\\config03_win.yaml\", \n",
    "                        help=\"Configuration\")\n",
    "    return parser.parse_args(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeNotExistDir(dirPath):\n",
    "    if not exists(dirPath):\n",
    "        pathlib.Path(dirPath).mkdir(parents=True, exist_ok=True) \n",
    "    return \"True\" if exists(dirPath) else \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: luna_detector/utils.py\n",
    "\n",
    "def getFreeId():\n",
    "    import pynvml \n",
    "\n",
    "    pynvml.nvmlInit()\n",
    "    def getFreeRatio(id):\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(id)\n",
    "        use = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "        ratio = 0.5*(float(use.gpu+float(use.memory)))\n",
    "        return ratio\n",
    "\n",
    "    deviceCount = pynvml.nvmlDeviceGetCount()\n",
    "    available = []\n",
    "    for i in range(deviceCount):\n",
    "        if getFreeRatio(i)<70:\n",
    "            available.append(i)\n",
    "    gpus = ''\n",
    "    for g in available:\n",
    "        gpus = gpus+str(g)+','\n",
    "    gpus = gpus[:-1]\n",
    "    return gpus\n",
    "\n",
    "def setgpu(gpuinput):\n",
    "    freeids = getFreeId()\n",
    "    if gpuinput=='all':\n",
    "        gpus = freeids\n",
    "    else:\n",
    "        gpus = gpuinput\n",
    "        gpus_in_use = [g not in freeids for g in gpus.split(',')]\n",
    "        if any(gpus_in_use):\n",
    "            raise ValueError('gpu'+gpus_in_use+'is being used')\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']=gpus\n",
    "    return gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: data_loader.py\n",
    "\n",
    "class LungNodule3Ddetector(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir, split_path, config, phase='train', split_comber=None):\n",
    "        assert(phase == 'train' or phase == 'val' or phase == 'test')\n",
    "        self.phase = phase\n",
    "        \n",
    "        self.max_stride = config['max_stride']\n",
    "        self.stride = config['stride']       \n",
    "        \n",
    "        sizelim = config['sizelim']/config['reso']\n",
    "        sizelim2 = config['sizelim2']/config['reso']\n",
    "        sizelim3 = config['sizelim3']/config['reso']\n",
    "        \n",
    "        self.blacklist = config['blacklist']\n",
    "        self.isScale = config['aug_scale']\n",
    "        self.r_rand  = config['r_rand_crop']\n",
    "        self.augtype = config['augtype']\n",
    "        self.pad_value = config['pad_value']\n",
    "\n",
    "        self.split_comber = split_comber\n",
    "        #idcs = np.load(split_path)\n",
    "        idcs = split_path\n",
    "\n",
    "        if phase != 'test':\n",
    "            idcs = [f for f in idcs if (f not in self.blacklist)]\n",
    "\n",
    "        self.filenames = [os.path.join(data_dir, '%s_clean.npy' % idx) for idx in idcs]\n",
    "        # display(self.filenames)\n",
    "        \n",
    "        labels = []\n",
    "        for idx in idcs:\n",
    "            l = np.load(os.path.join(data_dir, '%s_label.npy' %idx))\n",
    "            if np.all(l==0):\n",
    "                l=np.array([])\n",
    "            labels.append(l)\n",
    "        # display(labels)\n",
    "            \n",
    "        self.sample_bboxes = labels\n",
    "        if self.phase != 'test':\n",
    "            self.bboxes = []\n",
    "\n",
    "            for i, l in enumerate(labels):\n",
    "                if len(l) > 0 :\n",
    "                    for t in l:\n",
    "                        if t[3]>sizelim:\n",
    "                            self.bboxes+=[[np.concatenate([[i],t])]]\n",
    "                        if t[3]>sizelim2:\n",
    "                            self.bboxes+=[[np.concatenate([[i],t])]]*2\n",
    "                        if t[3]>sizelim3:\n",
    "                            self.bboxes+=[[np.concatenate([[i],t])]]*4\n",
    "            self.bboxes = np.concatenate(self.bboxes,axis = 0)\n",
    "\n",
    "        self.crop = Crop(config)\n",
    "        self.label_mapping = LabelMapping(config, self.phase)\n",
    "\n",
    "    def __getitem__(self, idx,split=None):\n",
    "        t = time.time()\n",
    "        np.random.seed(int(str(t%1)[2:7]))#seed according to time\n",
    "\n",
    "        isRandomImg  = False\n",
    "        if self.phase !='test':\n",
    "            if idx>=len(self.bboxes):\n",
    "                isRandom = True\n",
    "                idx = idx%len(self.bboxes)\n",
    "                isRandomImg = np.random.randint(2)\n",
    "            else:\n",
    "                isRandom = False\n",
    "        else:\n",
    "            isRandom = False\n",
    "        \n",
    "        if self.phase != 'test':\n",
    "            if not isRandomImg:\n",
    "                bbox = self.bboxes[idx]\n",
    "                filename = self.filenames[int(bbox[0])]\n",
    "                imgs = np.load(filename)\n",
    "                bboxes = self.sample_bboxes[int(bbox[0])]\n",
    "                isScale = self.augtype['scale'] and (self.phase=='train')\n",
    "                sample, target, bboxes, coord = self.crop(imgs, bbox[1:], bboxes,isScale,isRandom)\n",
    "                if self.phase=='train' and not isRandom:\n",
    "                     sample, target, bboxes, coord = augment(sample, target, bboxes, coord,\n",
    "                        ifflip = self.augtype['flip'], ifrotate=self.augtype['rotate'], ifswap = self.augtype['swap'])\n",
    "            else:\n",
    "                randimid = np.random.randint(len(self.filenames))\n",
    "                filename = self.filenames[randimid]\n",
    "                imgs = np.load(filename)\n",
    "                bboxes = self.sample_bboxes[randimid]\n",
    "                isScale = self.augtype['scale'] and (self.phase=='train')\n",
    "                sample, target, bboxes, coord = self.crop(imgs, [], bboxes, isScale=False, isRand=True)\n",
    "            label = self.label_mapping(sample.shape[1:], target, bboxes)\n",
    "            sample = (sample.astype(np.float32)-128)/128\n",
    "            return torch.from_numpy(sample), torch.from_numpy(label), coord\n",
    "        else:\n",
    "            imgs = np.load(self.filenames[idx])\n",
    "            bboxes = self.sample_bboxes[idx]\n",
    "            nz, nh, nw = imgs.shape[1:]\n",
    "            pz = int(np.ceil(float(nz) / self.stride)) * self.stride\n",
    "            ph = int(np.ceil(float(nh) / self.stride)) * self.stride\n",
    "            pw = int(np.ceil(float(nw) / self.stride)) * self.stride\n",
    "            imgs = np.pad(\n",
    "                imgs, \n",
    "                [[0,0], [0, pz-nz], [0, ph-nh], [0, pw-nw]], \n",
    "                'constant',\n",
    "                constant_values = self.pad_value\n",
    "            )\n",
    "            \n",
    "            xx, yy, zz = np.meshgrid(\n",
    "                np.linspace(-0.5, 0.5, int(imgs.shape[1]/self.stride)),\n",
    "                np.linspace(-0.5, 0.5, int(imgs.shape[2]/self.stride)),\n",
    "                np.linspace(-0.5, 0.5, int(imgs.shape[3]/self.stride)),\n",
    "                indexing ='ij'\n",
    "            )\n",
    "            coord = np.concatenate([xx[np.newaxis,...], yy[np.newaxis,...],zz[np.newaxis,:]],0).astype('float32')\n",
    "            \n",
    "            imgs, nzhw = self.split_comber.split(imgs)\n",
    "            \n",
    "            coord2, nzhw2 = self.split_comber.split(\n",
    "                coord,\n",
    "                side_len = int(self.split_comber.side_len/self.stride),\n",
    "                max_stride = int(self.split_comber.max_stride/self.stride),\n",
    "                margin = int(self.split_comber.margin/self.stride)\n",
    "            )\n",
    "            \n",
    "            assert np.all(nzhw==nzhw2)\n",
    "            imgs = (imgs.astype(np.float32)-128)/128\n",
    "            return torch.from_numpy(imgs), bboxes, torch.from_numpy(coord2), np.array(nzhw)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.phase == 'train':\n",
    "            return int(len(self.bboxes)/(1-self.r_rand))\n",
    "        elif self.phase =='val':\n",
    "            return len(self.bboxes)\n",
    "        else:\n",
    "            return len(self.sample_bboxes)\n",
    "       \n",
    "    \n",
    "def augment(sample, target, bboxes, coord, ifflip=True, ifrotate=True, ifswap=True):\n",
    "    #                     angle1 = np.random.rand()*180\n",
    "    if ifrotate:\n",
    "        validrot = False\n",
    "        counter = 0\n",
    "        while not validrot:\n",
    "            newtarget = np.copy(target)\n",
    "            angle1 = (np.random.rand() - 0.5) * 20\n",
    "            size = np.array(sample.shape[2:4]).astype('float')\n",
    "            rotmat = np.array([[np.cos(angle1 / 180 * np.pi), -np.sin(angle1 / 180 * np.pi)],\n",
    "                               [np.sin(angle1 / 180 * np.pi), np.cos(angle1 / 180 * np.pi)]])\n",
    "            newtarget[1:3] = np.dot(rotmat, target[1:3] - size / 2) + size / 2\n",
    "            if np.all(newtarget[:3] > target[3]) and np.all(newtarget[:3] < np.array(sample.shape[1:4]) - newtarget[3]):\n",
    "                validrot = True\n",
    "                target = newtarget\n",
    "                sample = rotate(sample, angle1, axes=(2, 3), reshape=False)\n",
    "                coord = rotate(coord, angle1, axes=(2, 3), reshape=False)\n",
    "                for box in bboxes:\n",
    "                    box[1:3] = np.dot(rotmat, box[1:3] - size / 2) + size / 2\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter == 3:\n",
    "                    break\n",
    "    if ifswap:\n",
    "        if sample.shape[1] == sample.shape[2] and sample.shape[1] == sample.shape[3]:\n",
    "            axisorder = np.random.permutation(3)\n",
    "            sample = np.transpose(sample, np.concatenate([[0], axisorder + 1]))\n",
    "            coord = np.transpose(coord, np.concatenate([[0], axisorder + 1]))\n",
    "            target[:3] = target[:3][axisorder]\n",
    "            bboxes[:, :3] = bboxes[:, :3][:, axisorder]\n",
    "\n",
    "    if ifflip:\n",
    "        #         flipid = np.array([np.random.randint(2),np.random.randint(2),np.random.randint(2)])*2-1\n",
    "        flipid = np.array([1, np.random.randint(2), np.random.randint(2)]) * 2 - 1\n",
    "        sample = np.ascontiguousarray(sample[:, ::flipid[0], ::flipid[1], ::flipid[2]])\n",
    "        coord = np.ascontiguousarray(coord[:, ::flipid[0], ::flipid[1], ::flipid[2]])\n",
    "        for ax in range(3):\n",
    "            if flipid[ax] == -1:\n",
    "                target[ax] = np.array(sample.shape[ax + 1]) - target[ax]\n",
    "                bboxes[:, ax] = np.array(sample.shape[ax + 1]) - bboxes[:, ax]\n",
    "    return sample, target, bboxes, coord    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: data_loader.py\n",
    "\n",
    "class Crop(object):\n",
    "    def __init__(self, config):\n",
    "        self.crop_size = config['crop_size']\n",
    "        self.bound_size = config['bound_size']\n",
    "        self.stride = config['stride']\n",
    "        self.pad_value = config['pad_value']\n",
    "\n",
    "    def __call__(self, imgs, target, bboxes, isScale=False, isRand=False):\n",
    "        if isScale:\n",
    "            radiusLim = [8., 100.]\n",
    "            scaleLim = [0.75, 1.25]\n",
    "            scaleRange = [np.min([np.max([(radiusLim[0] / target[3]), scaleLim[0]]), 1])\n",
    "                , np.max([np.min([(radiusLim[1] / target[3]), scaleLim[1]]), 1])]\n",
    "            scale = np.random.rand() * (scaleRange[1] - scaleRange[0]) + scaleRange[0]\n",
    "            crop_size = (np.array(self.crop_size).astype('float') / scale).astype('int')\n",
    "        else:\n",
    "            crop_size = self.crop_size\n",
    "        bound_size = self.bound_size\n",
    "        target = np.copy(target)\n",
    "        bboxes = np.copy(bboxes)\n",
    "\n",
    "        start = []\n",
    "        for i in range(3):\n",
    "            if not isRand:\n",
    "                r = target[3] / 2\n",
    "                s = np.floor(target[i] - r) + 1 - bound_size\n",
    "                e = np.ceil(target[i] + r) + 1 + bound_size - crop_size[i]\n",
    "            else:\n",
    "                s = np.max([imgs.shape[i + 1] - crop_size[i] / 2, imgs.shape[i + 1] / 2 + bound_size])\n",
    "                e = np.min([crop_size[i] / 2, imgs.shape[i + 1] / 2 - bound_size])\n",
    "                target = np.array([np.nan, np.nan, np.nan, np.nan])\n",
    "            if s > e:\n",
    "                start.append(int(np.random.randint(e, s)))  # !\n",
    "            else:\n",
    "                start.append(int(target[i] - crop_size[i] / 2 + np.random.randint(-bound_size / 2, bound_size / 2)))\n",
    "\n",
    "        normstart = np.array(start).astype('float32') / np.array(imgs.shape[1:]) - 0.5\n",
    "        normsize = np.array(crop_size).astype('float32') / np.array(imgs.shape[1:])\n",
    "        xx, yy, zz = np.meshgrid(\n",
    "            np.linspace(normstart[0], normstart[0] + normsize[0], int(self.crop_size[0]/self.stride)),\n",
    "            np.linspace(normstart[1], normstart[1] + normsize[1], int(self.crop_size[1]/self.stride)),\n",
    "            np.linspace(normstart[2], normstart[2] + normsize[2], int(self.crop_size[2]/self.stride)),\n",
    "            indexing='ij')\n",
    "        \n",
    "        coord = np.concatenate([xx[np.newaxis, ...], yy[np.newaxis, ...], zz[np.newaxis, :]], 0).astype('float32')\n",
    "\n",
    "        pad = []\n",
    "        pad.append([0, 0])\n",
    "        for i in range(3):\n",
    "            leftpad = max(0, -start[i])\n",
    "            rightpad = max(0, start[i] + crop_size[i] - imgs.shape[i + 1])\n",
    "            pad.append([leftpad, rightpad])\n",
    "        crop = imgs[:,\n",
    "               max(start[0], 0):min(start[0] + crop_size[0], imgs.shape[1]),\n",
    "               max(start[1], 0):min(start[1] + crop_size[1], imgs.shape[2]),\n",
    "               max(start[2], 0):min(start[2] + crop_size[2], imgs.shape[3])]\n",
    "        crop = np.pad(crop, pad, 'constant', constant_values=self.pad_value)\n",
    "        for i in range(3):\n",
    "            target[i] = target[i] - start[i]\n",
    "        for i in range(len(bboxes)):\n",
    "            for j in range(3):\n",
    "                bboxes[i][j] = bboxes[i][j] - start[j]\n",
    "\n",
    "        if isScale:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                crop = zoom(crop, [1, scale, scale, scale], order=1)\n",
    "            newpad = self.crop_size[0] - crop.shape[1:][0]\n",
    "            if newpad < 0:\n",
    "                crop = crop[:, :-newpad, :-newpad, :-newpad]\n",
    "            elif newpad > 0:\n",
    "                pad2 = [[0, 0], [0, newpad], [0, newpad], [0, newpad]]\n",
    "                crop = np.pad(crop, pad2, 'constant', constant_values=self.pad_value)\n",
    "            for i in range(4):\n",
    "                target[i] = target[i] * scale\n",
    "            for i in range(len(bboxes)):\n",
    "                for j in range(4):\n",
    "                    bboxes[i][j] = bboxes[i][j] * scale\n",
    "        return crop, target, bboxes, coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: data_loader.py\n",
    "\n",
    "class LabelMapping(object):\n",
    "    def __init__(self, config, phase):\n",
    "        self.stride = np.array(config['stride'])\n",
    "        self.num_neg = int(config['num_neg'])\n",
    "        self.th_neg = config['th_neg']\n",
    "        self.anchors = np.asarray(config['anchors'])\n",
    "        self.phase = phase\n",
    "        \n",
    "        if phase == 'train':\n",
    "            self.th_pos = config['th_pos_train']\n",
    "        elif phase == 'val':\n",
    "            self.th_pos = config['th_pos_val']\n",
    "\n",
    "    def __call__(self, input_size, target, bboxes):\n",
    "        stride = self.stride\n",
    "        num_neg = self.num_neg\n",
    "        th_neg = self.th_neg\n",
    "        anchors = self.anchors\n",
    "        th_pos = self.th_pos\n",
    "        struct = generate_binary_structure(3, 1)\n",
    "\n",
    "        output_size = []\n",
    "        for i in range(3):\n",
    "            assert (input_size[i] % stride == 0)\n",
    "            output_size.append(int(input_size[i] / stride))\n",
    "\n",
    "        label = np.zeros(output_size + [len(anchors), 5], np.float32)\n",
    "        offset = ((stride.astype('float')) - 1) / 2\n",
    "        oz = np.arange(offset, offset + stride * (output_size[0] - 1) + 1, stride)\n",
    "        oh = np.arange(offset, offset + stride * (output_size[1] - 1) + 1, stride)\n",
    "        ow = np.arange(offset, offset + stride * (output_size[2] - 1) + 1, stride)\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            for i, anchor in enumerate(anchors):\n",
    "                iz, ih, iw = select_samples(bbox, anchor, th_neg, oz, oh, ow)\n",
    "                label[iz, ih, iw, i, 0] = 1\n",
    "                label[:, :, :, i, 0] = binary_dilation(label[:, :, :, i, 0].astype('bool'), structure=struct,\n",
    "                                                       iterations=1).astype('float32')\n",
    "\n",
    "        label = label - 1\n",
    "\n",
    "        if self.phase == 'train' and self.num_neg > 0:\n",
    "            neg_z, neg_h, neg_w, neg_a = np.where(label[:, :, :, :, 0] == -1)\n",
    "            neg_idcs = random.sample(range(len(neg_z)), min(num_neg, len(neg_z)))\n",
    "            neg_z, neg_h, neg_w, neg_a = neg_z[neg_idcs], neg_h[neg_idcs], neg_w[neg_idcs], neg_a[neg_idcs]\n",
    "            label[:, :, :, :, 0] = 0\n",
    "            label[neg_z, neg_h, neg_w, neg_a, 0] = -1\n",
    "\n",
    "        if np.isnan(target[0]):\n",
    "            return label\n",
    "        iz, ih, iw, ia = [], [], [], []\n",
    "        for i, anchor in enumerate(anchors):\n",
    "            iiz, iih, iiw = select_samples(target, anchor, th_pos, oz, oh, ow)\n",
    "            iz.append(iiz)\n",
    "            ih.append(iih)\n",
    "            iw.append(iiw)\n",
    "            ia.append(i * np.ones((len(iiz),), np.int64))\n",
    "        iz = np.concatenate(iz, 0)\n",
    "        ih = np.concatenate(ih, 0)\n",
    "        iw = np.concatenate(iw, 0)\n",
    "        ia = np.concatenate(ia, 0)\n",
    "        flag = True\n",
    "        if len(iz) == 0:\n",
    "            pos = []\n",
    "            for i in range(3):\n",
    "                pos.append(max(0, int(np.round((target[i] - offset) / stride))))\n",
    "            idx = np.argmin(np.abs(np.log(target[3] / anchors)))\n",
    "            pos.append(idx)\n",
    "            flag = False\n",
    "        else:\n",
    "            idx = random.sample(range(len(iz)), 1)[0]\n",
    "            pos = [iz[idx], ih[idx], iw[idx], ia[idx]]\n",
    "        dz = (target[0] - oz[pos[0]]) / anchors[pos[3]]\n",
    "        dh = (target[1] - oh[pos[1]]) / anchors[pos[3]]\n",
    "        dw = (target[2] - ow[pos[2]]) / anchors[pos[3]]\n",
    "        dd = np.log(target[3] / anchors[pos[3]])\n",
    "        label[pos[0], pos[1], pos[2], pos[3], :] = [1, dz, dh, dw, dd]\n",
    "        return label\n",
    "\n",
    "\n",
    "def select_samples(bbox, anchor, th, oz, oh, ow):\n",
    "    z, h, w, d = bbox\n",
    "    max_overlap = min(d, anchor)\n",
    "    min_overlap = np.power(max(d, anchor), 3) * th / max_overlap / max_overlap\n",
    "    if min_overlap > max_overlap:\n",
    "        return np.zeros((0,), np.int64), np.zeros((0,), np.int64), np.zeros((0,), np.int64)\n",
    "    else:\n",
    "        s = z - 0.5 * np.abs(d - anchor) - (max_overlap - min_overlap)\n",
    "        e = z + 0.5 * np.abs(d - anchor) + (max_overlap - min_overlap)\n",
    "        mz = np.logical_and(oz >= s, oz <= e)\n",
    "        iz = np.where(mz)[0]\n",
    "\n",
    "        s = h - 0.5 * np.abs(d - anchor) - (max_overlap - min_overlap)\n",
    "        e = h + 0.5 * np.abs(d - anchor) + (max_overlap - min_overlap)\n",
    "        mh = np.logical_and(oh >= s, oh <= e)\n",
    "        ih = np.where(mh)[0]\n",
    "\n",
    "        s = w - 0.5 * np.abs(d - anchor) - (max_overlap - min_overlap)\n",
    "        e = w + 0.5 * np.abs(d - anchor) + (max_overlap - min_overlap)\n",
    "        mw = np.logical_and(ow >= s, ow <= e)\n",
    "        iw = np.where(mw)[0]\n",
    "\n",
    "        if len(iz) == 0 or len(ih) == 0 or len(iw) == 0:\n",
    "            return np.zeros((0,), np.int64), np.zeros((0,), np.int64), np.zeros((0,), np.int64)\n",
    "\n",
    "        lz, lh, lw = len(iz), len(ih), len(iw)\n",
    "        iz = iz.reshape((-1, 1, 1))\n",
    "        ih = ih.reshape((1, -1, 1))\n",
    "        iw = iw.reshape((1, 1, -1))\n",
    "        iz = np.tile(iz, (1, lh, lw)).reshape((-1))\n",
    "        ih = np.tile(ih, (lz, 1, lw)).reshape((-1))\n",
    "        iw = np.tile(iw, (lz, lh, 1)).reshape((-1))\n",
    "        centers = np.concatenate([\n",
    "            oz[iz].reshape((-1, 1)),\n",
    "            oh[ih].reshape((-1, 1)),\n",
    "            ow[iw].reshape((-1, 1))], axis=1)\n",
    "\n",
    "        r0 = anchor / 2\n",
    "        s0 = centers - r0\n",
    "        e0 = centers + r0\n",
    "\n",
    "        r1 = d / 2\n",
    "        s1 = bbox[:3] - r1\n",
    "        s1 = s1.reshape((1, -1))\n",
    "        e1 = bbox[:3] + r1\n",
    "        e1 = e1.reshape((1, -1))\n",
    "\n",
    "        overlap = np.maximum(0, np.minimum(e0, e1) - np.maximum(s0, s1))\n",
    "\n",
    "        intersection = overlap[:, 0] * overlap[:, 1] * overlap[:, 2]\n",
    "        union = anchor * anchor * anchor + d * d * d - intersection\n",
    "\n",
    "        iou = intersection / union\n",
    "\n",
    "        mask = iou >= th\n",
    "        # if th > 0.4:\n",
    "        #   if np.sum(mask) == 0:\n",
    "        #      print(['iou not large', iou.max()])\n",
    "        # else:\n",
    "        #    print(['iou large', iou[mask]])\n",
    "        iz = iz[mask]\n",
    "        ih = ih[mask]\n",
    "        iw = iw[mask]\n",
    "        return iz, ih, iw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, net, loss, epoch, optimizer, get_lr, save_dir):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    net.train()\n",
    "    lr = get_lr(epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    metrics = []\n",
    "    for i, (data, target, coord) in enumerate(data_loader):\n",
    "        data = Variable(data.cuda())\n",
    "        target = Variable(target.cuda())\n",
    "        coord = Variable(coord.cuda())\n",
    "\n",
    "        output = net(data, coord)\n",
    "        \n",
    "        loss_output = loss(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss_output[0].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_output[0] = loss_output[0].item()\n",
    "        metrics.append(loss_output)\n",
    "        \n",
    "        print(\"finished iteration {} with loss {}.\".format(i, loss_output[0]))\n",
    "                \n",
    "    end_time = time.time()\n",
    "\n",
    "    metrics = np.asarray(metrics, np.float32)\n",
    "    print('\\nEpoch %03d (lr %.5f)' % (epoch, lr))\n",
    "    print('Train:      tpr %3.2f, tnr %3.2f, total pos %d, total neg %d, time %3.2f' % (\n",
    "        100.0 * np.sum(metrics[:, 6]) / np.sum(metrics[:, 7]),\n",
    "        100.0 * np.sum(metrics[:, 8]) / np.sum(metrics[:, 9]),\n",
    "        np.sum(metrics[:, 7]),\n",
    "        np.sum(metrics[:, 9]),\n",
    "        end_time - start_time))\n",
    "    print('loss %2.4f, classify loss %2.4f, regress loss %2.4f, %2.4f, %2.4f, %2.4f' % (\n",
    "        np.mean(metrics[:, 0]),\n",
    "        np.mean(metrics[:, 1]),\n",
    "        np.mean(metrics[:, 2]),\n",
    "        np.mean(metrics[:, 3]),\n",
    "        np.mean(metrics[:, 4]),\n",
    "        np.mean(metrics[:, 5])))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(data_loader, net, loss):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    metrics = []\n",
    "    for i, (data, target, coord) in enumerate(data_loader):\n",
    "        data = Variable(data.cuda())\n",
    "        target = Variable(target.cuda())\n",
    "        coord = Variable(coord.cuda())\n",
    "\n",
    "        output = net(data, coord)\n",
    "        loss_output = loss(output, target, train = False)\n",
    "\n",
    "        loss_output[0] = loss_output[0].item()\n",
    "        metrics.append(loss_output)    \n",
    "    end_time = time.time()\n",
    "\n",
    "    metrics = np.asarray(metrics, np.float32)\n",
    "    print('\\nValidation: tpr %3.2f, tnr %3.8f, total pos %d, total neg %d, time %3.2f' % (\n",
    "        100.0 * np.sum(metrics[:, 6]) / np.sum(metrics[:, 7]),\n",
    "        100.0 * np.sum(metrics[:, 8]) / np.sum(metrics[:, 9]),\n",
    "        np.sum(metrics[:, 7]),\n",
    "        np.sum(metrics[:, 9]),\n",
    "        end_time - start_time))\n",
    "    print('loss %2.4f, classify loss %2.4f, regress loss %2.4f, %2.4f, %2.4f, %2.4f' % (\n",
    "        np.mean(metrics[:, 0]),\n",
    "        np.mean(metrics[:, 1]),\n",
    "        np.mean(metrics[:, 2]),\n",
    "        np.mean(metrics[:, 3]),\n",
    "        np.mean(metrics[:, 4]),\n",
    "        np.mean(metrics[:, 5])))\n",
    "    return np.mean(metrics[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main\n",
    "args = parse_args(args=[])\n",
    "print(args)\n",
    "    \n",
    "path_root = RootPath\n",
    "path_conf = join(path_root, args.cfg)\n",
    "    \n",
    "CFG = get_cfg_defaults()\n",
    "CFG.merge_from_file(path_conf)\n",
    "CFG.freeze()\n",
    "print(); print(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ckpt = join(path_root, CFG.TRAIN.DIR_CKPT)\n",
    "isExist = makeNotExistDir(path_ckpt)\n",
    "print(\"'{}' exist? {}\".format(path_ckpt, isExist))\n",
    "\n",
    "path_save = join(path_root, CFG.TRAIN.DIR_SAVE)\n",
    "isExist = makeNotExistDir(path_save)\n",
    "print(\"'{}' exist? {}\".format(path_save, isExist))\n",
    "\n",
    "path_data = join(path_root, CFG.TRAIN.DIR_DATA)\n",
    "isExist = makeNotExistDir(path_data)\n",
    "print(\"'{}' exist? {}\".format(path_data, isExist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = import_module(CFG.TRAIN.MODEL)\n",
    "config, net, loss, get_pbb = model.get_model()\n",
    "\n",
    "print(type(config))\n",
    "display(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "start_epoch = CFG.TRAIN.START_EPOCH\n",
    "\n",
    "if CFG.TRAIN.RESUME:\n",
    "    abspath_ckpt = join(path_ckpt, 'detector_'+CFG.TRAIN.RESUME)\n",
    "    if exists(abspath_ckpt):\n",
    "        print(\"\\nLoad checkpoint: '{}'\".format(abspath_ckpt))\n",
    "        checkpoint = torch.load(abspath_ckpt)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        net.load_state_dict(checkpoint['state_dict'])\n",
    "    else:\n",
    "        print(\"\\n'{}' not exist.\".format(abspath_ckpt))\n",
    "    \n",
    "print('Start epoch: {}'.format(start_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU \n",
    "gpus = setgpu(CFG.TRAIN.GPU)\n",
    "net  = net.cuda()\n",
    "loss = loss.cuda()\n",
    "cudnn.benchmark = True\n",
    "net = DataParallel(net)\n",
    "\n",
    "n_gpu = len(gpus.split(','))\n",
    "print('Using GPUs:', n_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test list\n",
    "name_series = pd.Series(\n",
    "    glob(path_data + \"\\\\*label.npy\")\n",
    ").apply(lambda x: x.split(\"_label.npy\")[0].split(\"\\\\\")[-1])\n",
    "\n",
    "name_list = name_series.values.tolist()\n",
    "\n",
    "frac_for_train = CFG.TRAIN.FRAC_FOR_TRAIN\n",
    "len_train = int(len(name_list) * frac_for_train)\n",
    "\n",
    "luna_train = name_list[:len_train]\n",
    "luna_valid = name_list[len_train:]\n",
    "\n",
    "print('Train dset:', luna_train)\n",
    "print('Valid dset:', luna_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = LungNodule3Ddetector(path_data, luna_train, config, \"train\")\n",
    "\n",
    "trainSize = len(trainSet)\n",
    "print('Dataset size:', trainSize)\n",
    "\n",
    "imgs, label, coord = trainSet[0]\n",
    "print('\\nImage:', imgs.shape)\n",
    "print('Label:', label.shape)\n",
    "print('coord:', coord.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = CFG.TRAIN.BATCH_SIZE\n",
    "num_workers = CFG.TRAIN.NUM_WORKERS\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    trainSet, \n",
    "    batch_size = batch_size, \n",
    "    shuffle = True, \n",
    "    num_workers = num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "imgs, lbls, cords = next(iter(train_loader))\n",
    "print('\\nImage type:', type(imgs))\n",
    "print('      size: ', imgs.size())\n",
    "\n",
    "print('\\nLabel type:', type(lbls))\n",
    "print('      size: ', lbls.size())\n",
    "\n",
    "print('\\nCoord type:', type(cords))\n",
    "print('      size: ', cords.size())\n",
    "\n",
    "#img = imgs[0]\n",
    "#print('\\nImage shape:', img.shape)\n",
    "#print(); print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validSet = LungNodule3Ddetector(path_data, luna_valid, config, phase='val')\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    validSet, \n",
    "    batch_size = batch_size, \n",
    "    shuffle = False, \n",
    "    num_workers = num_workers, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestLoss = 1000\n",
    "    \n",
    "torch.manual_seed(0)\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "NumEpochs = CFG.TRAIN.NUM_EPOCHS\n",
    "LR = float(CFG.TRAIN.LEARNING_RATE)\n",
    "mo = float(CFG.TRAIN.MOMENTUM)\n",
    "wd = float(CFG.TRAIN.WEIGHT_DECAY)\n",
    "\n",
    "optimizer = torch.optim.SGD( net.parameters(), LR, momentum=mo, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_lr(epoch):\n",
    "    if epoch <= NumEpochs * 0.2:\n",
    "        lr = LR\n",
    "    elif epoch <= NumEpochs * 0.4:\n",
    "        lr = 0.1 * LR\n",
    "    elif epoch <= NumEpochs * 0.6:\n",
    "        lr = 0.05 * LR\n",
    "    else:\n",
    "        lr = 0.01 * LR\n",
    "    return lr\n",
    "\n",
    "for epoch in range(start_epoch, NumEpochs+1):\n",
    "    train(train_loader, net, loss, epoch, optimizer, get_lr, path_save)\n",
    "    print(\"finsihed epoch {}\".format(epoch))\n",
    "    \n",
    "    valiloss = validate(valid_loader, net, loss)\n",
    "    \n",
    "    if bestLoss > valiloss:   \n",
    "        bestLoss = valiloss\n",
    "        state_dict = net.module.state_dict()\n",
    "        for key in state_dict.keys():\n",
    "            state_dict[key] = state_dict[key].cpu()\n",
    "                \n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'save_dir': path_save,\n",
    "            'state_dict': state_dict,\n",
    "            'args': args},\n",
    "            os.path.join(path_save, 'detector_%03d.ckpt' % epoch))\n",
    "        print(\"\\nSave model on epoch %d\" % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
